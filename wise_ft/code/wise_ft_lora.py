import torch
import numpy as np
import json
import os
from safetensors.torch import load_file, save_file
from collections import OrderedDict
import copy
import argparse
from typing import Dict, Optional, Tuple, List

# ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì„¤ì •
def setup_device():
    """ì•ˆì „í•œ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
    if torch.cuda.is_available():
        try:
            test_tensor = torch.tensor([1.0]).to("cuda")
            print(f"CUDA available and working: {torch.cuda.get_device_name()}")
            return torch.device("cuda")
        except Exception as e:
            print(f"CUDA available but not working: {e}")
            print("Falling back to CPU...")
            return torch.device("cpu")
    else:
        print("â„¹ï¸ CUDA not available, using CPU")
        return torch.device("cpu")

device = setup_device()
EPS = 1e-8

def load_lora_weights(model_path: str) -> Dict[str, torch.Tensor]:
    """LoRA adapter ê°€ì¤‘ì¹˜ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    adapter_path = os.path.join(model_path, 'adapter_model.safetensors')
    if os.path.exists(adapter_path):
        try:
            print(f"Loading {adapter_path}...")
            weights = load_file(adapter_path, device="cpu")
            print(f"âœ… Loaded to CPU: {len(weights)} parameters")
            
            # ì•ˆì „í•˜ê²Œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            global device
            if device.type == "cuda":
                try:
                    weights = {k: v.to(device) for k, v in weights.items()}
                    print(f"âœ… Moved to GPU: {device}")
                except Exception as e:
                    print(f"âš ï¸ Failed to move to GPU: {e}")
                    print("Keeping weights on CPU...")
                    device = torch.device("cpu")
            
            total_params = sum(t.numel() for t in weights.values())
            print(f"ğŸ“Š Total parameters: {total_params:,}")
            
            return weights
        except Exception as e:
            print(f"âŒ Error loading {adapter_path}: {e}")
            raise
    else:
        raise FileNotFoundError(f"adapter_model.safetensors not found in {model_path}")

def load_config(model_path: str) -> Dict:
    """LoRA adapter ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    config_path = os.path.join(model_path, 'adapter_config.json')
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        return config
    else:
        print(f"Warning: adapter_config.json not found in {model_path}")
        return {}

def compute_fisher_information(lora_weights: Dict[str, torch.Tensor], 
                             method: str = 'diagonal') -> Dict[str, torch.Tensor]:
    """
    LoRA adapterì— ëŒ€í•œ Fisher Informationì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    (ì‹¤ì œ ë°ì´í„° ì—†ì´ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê·¼ì‚¬)
    """
    print(f"ğŸ§® Computing Fisher Information using {method} method")
    
    fisher_info = {}
    
    if method == 'diagonal':
        for key, weight in lora_weights.items():
            if 'lora_A' in key or 'lora_B' in key:
                # LoRA íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜ Fisher
                fisher_info[key] = torch.abs(weight) + EPS
            else:
                fisher_info[key] = torch.ones_like(weight) * 0.1
    
    elif method == 'empirical':
        for key, weight in lora_weights.items():
            fisher_info[key] = torch.abs(weight) ** 0.5 + EPS
    
    elif method == 'uniform':
        for key, weight in lora_weights.items():
            fisher_info[key] = torch.ones_like(weight)
    
    else:
        raise ValueError(f"Unknown Fisher method: {method}")
    
    print(f"âœ… Fisher Information computed for {len(fisher_info)} parameters")
    return fisher_info

def _merge(alpha: float, 
          theta_0: Dict[str, torch.Tensor], 
          theta_1: Dict[str, torch.Tensor], 
          fishers: Optional[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]] = None,
          fisher_floor: float = 1e-8) -> Dict[str, torch.Tensor]:
    """
    ì›ë³¸ wise_ftì˜ _merge í•¨ìˆ˜ë¥¼ LoRAìš©ìœ¼ë¡œ êµ¬í˜„
    
    Args:
        alpha: ë³´ê°„ íŒŒë¼ë¯¸í„° (0: zeroshot only, 1: finetuned only)
        theta_0: Zero-shot LoRA weights
        theta_1: Fine-tuned LoRA weights  
        fishers: Fisher Information (fisher_0, fisher_1) - Optional
        fisher_floor: Fisher Informationì˜ ìµœì†Œê°’
    
    Returns:
        Dict[str, torch.Tensor]: ë³‘í•©ëœ LoRA weights
    """
    
    if fishers is None:
        # ì›ë³¸ wise_ftì˜ ê¸°ë³¸ ë¡œì§: ë‹¨ìˆœ ì„ í˜• ë³´ê°„
        print(f"ğŸ“ Simple linear interpolation with alpha={alpha:.3f}")
        return {
            key: (1 - alpha) * theta_0[key] + alpha * theta_1[key]
            for key in theta_0.keys()
        }

    # Fisher Informationì„ ì‚¬ìš©í•œ ê°€ì¤‘ ë³‘í•© (wise_ft ê³ ê¸‰ ê¸°ëŠ¥)
    print(f"ğŸ§® Fisher-weighted interpolation with alpha={alpha:.3f}")
    fisher_0, fisher_1 = fishers

    theta = {}
    for key in theta_0.keys():
        # Fisher Information í™•ì¸ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•œ assert)
        assert (key in fisher_0) == (key in fisher_1), f"Fisher mismatch for {key}"
        
        ones = torch.ones_like(theta_0[key])
        f_0 = torch.maximum(fisher_0.get(key, ones), fisher_floor * ones)
        f_1 = torch.maximum(fisher_1.get(key, ones), fisher_floor * ones)

        c_0 = (1 - alpha) * f_0
        c_1 = alpha * f_1

        theta[key] = (c_0 * theta_0[key] + c_1 * theta_1[key]) / (c_0 + c_1)

    return theta

def save_merged_weights(merged_weights: Dict[str, torch.Tensor], 
                       output_path: str, 
                       source_config_path: str):
    """ë³‘í•©ëœ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(output_path, exist_ok=True)
    
    try:
        # safetensors í˜•ì‹ìœ¼ë¡œ ì €ì¥
        adapter_path = os.path.join(output_path, 'adapter_model.safetensors')
        
        # CPUë¡œ ì´ë™ í›„ ì €ì¥
        cpu_weights = {k: v.cpu() for k, v in merged_weights.items()}
        save_file(cpu_weights, adapter_path)
        
        print(f"âœ… Merged weights saved to {adapter_path}")
        
        # ì„¤ì • íŒŒì¼ë“¤ë„ ë³µì‚¬
        config_files = ['adapter_config.json', 'config.json']
        for config_file in config_files:
            source_config = os.path.join(source_config_path, config_file)
            target_config = os.path.join(output_path, config_file)
            
            if os.path.exists(source_config):
                import shutil
                shutil.copy2(source_config, target_config)
                print(f"âœ… Copied {config_file}")
        
    except Exception as e:
        print(f"âŒ Error saving weights: {e}")
        raise

def wise_ft(args):
    """
    ì›ë³¸ wise_ft í•¨ìˆ˜ì˜ LoRA ë²„ì „
    """
    print("ğŸš€ Wise-FT for LoRA Adapters")
    print("=" * 60)
    print(f"Device: {device}")
    if args.zeroshot_path:
        print(f"Zero-shot model: {args.zeroshot_path}")
    else:
        print("Zero-shot model: Empty LoRA (all zeros)")
    print(f"Fine-tuned model: {args.finetuned_path}")
    print(f"Output directory: {args.save}")
    print(f"Alpha values: {args.alpha}")
    print(f"Use Fisher: {args.fisher is not None}")
    print("=" * 60)
    
    # Load models
    print("\nğŸ“¥ STEP 1: Loading LoRA adapters")
    
    # Fine-tuned LoRAëŠ” ë°˜ë“œì‹œ ë¡œë“œ
    theta_1 = load_lora_weights(args.finetuned_path)   # finetuned LoRA
    
    # Zero-shot LoRA ì²˜ë¦¬
    if args.zeroshot_path and os.path.exists(os.path.join(args.zeroshot_path, 'adapter_model.safetensors')):
        # ì‹¤ì œ zero-shot LoRA adapterê°€ ìˆëŠ” ê²½ìš°
        theta_0 = load_lora_weights(args.zeroshot_path)
        print("âœ… Loaded actual zero-shot LoRA adapter")
    else:
        # Zero-shot = ë¹ˆ LoRA (ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ 0)
        print("ğŸ“ Creating zero-shot LoRA state (all zeros)")
        theta_0 = {key: torch.zeros_like(weight) for key, weight in theta_1.items()}
        print(f"âœ… Created zero-shot state with {len(theta_0)} zero-initialized parameters")
    
    # Make sure checkpoints are compatible
    print("\nğŸ” STEP 2: Checking compatibility")
    assert set(theta_0.keys()) == set(theta_1.keys()), \
        "LoRA adapters have incompatible keys!"
    print(f"âœ… Both adapters have {len(theta_0)} compatible parameters")
    
    # Fisher Information ì²˜ë¦¬ (ì›ë³¸ ì½”ë“œ ë¡œì§)
    fishers = None
    if args.fisher is not None:
        print(f"\nğŸ§® STEP 3: Computing Fisher Information")
        if args.zeroshot_path and os.path.exists(os.path.join(args.zeroshot_path, 'adapter_model.safetensors')):
            fisher_0 = compute_fisher_information(theta_0, method=args.fisher_method)
        else:
            # Zero-shotì€ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ 0ì´ë¯€ë¡œ ê· ë“±í•œ Fisher ì‚¬ìš©
            fisher_0 = {key: torch.ones_like(weight) * args.fisher_floor 
                       for key, weight in theta_0.items()}
        
        fisher_1 = compute_fisher_information(theta_1, method=args.fisher_method)
        fishers = fisher_0, fisher_1
    else:
        print("\nğŸ“ STEP 3: Skipping Fisher Information (using simple interpolation)")
    
    # Interpolate between checkpoints for each alpha (ì›ë³¸ ì½”ë“œ ë¡œì§)
    print(f"\nğŸ”„ STEP 4: Interpolating for {len(args.alpha)} alpha values")
    results = {}
    
    for alpha in args.alpha:
        print(f"\nğŸ”„ Processing alpha = {alpha:.3f}")
        
        # ì›ë³¸ wise_ftì˜ í•µì‹¬: _merge í•¨ìˆ˜ í˜¸ì¶œ
        theta = _merge(alpha, theta_0, theta_1, fishers, args.fisher_floor)
        
        # Save model (ì›ë³¸ì—ì„œëŠ” finetuned.save(), ì—¬ê¸°ì„œëŠ” LoRA adapter ì €ì¥)
        output_path = os.path.join(args.save, f'wise_ft_alpha_{alpha:.3f}')
        save_merged_weights(theta, output_path, args.finetuned_path)
        
        # ê²°ê³¼ ê¸°ë¡
        results[alpha] = {
            'param_count': len(theta),
            'output_path': output_path
        }
        
        print(f"âœ… Alpha {alpha:.3f} completed: {len(theta)} parameters")
        
        # TODO: ì—¬ê¸°ì— evaluate() í•¨ìˆ˜ í˜¸ì¶œ ì¶”ê°€ ê°€ëŠ¥
        # evaluate(merged_adapter, args)
    
    # ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
    print(f"\nğŸ“‹ STEP 5: Creating analysis report")
    create_analysis_report(args, results)
    
    print(f"\nğŸ‰ Wise-FT completed successfully!")
    print(f"ğŸ“ Results saved in: {args.save}")
    
    return results

def create_analysis_report(args, results: Dict):
    """ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    from datetime import datetime
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'device': str(device),
        'method': 'Wise-FT for LoRA Adapters',
        'zeroshot_path': args.zeroshot_path,
        'finetuned_path': args.finetuned_path,
        'use_fisher': args.fisher is not None,
        'fisher_method': args.fisher_method if args.fisher else 'none',
        'alphas_tested': args.alpha,
        'results': results
    }
    
    # JSON ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(args.save, 'wise_ft_analysis.json')
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    # í…ìŠ¤íŠ¸ ìš”ì•½ ì €ì¥
    summary_path = os.path.join(args.save, 'wise_ft_summary.txt')
    with open(summary_path, 'w') as f:
        f.write("Wise-FT Analysis Summary (LoRA Version)\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Device: {device}\n")
        f.write(f"Method: Wise-FT for LoRA Adapters\n")
        f.write(f"Zero-shot model: {args.zeroshot_path}\n")
        f.write(f"Fine-tuned model: {args.finetuned_path}\n")
        f.write(f"Use Fisher Information: {args.fisher is not None}\n")
        f.write(f"Alpha values tested: {args.alpha}\n\n")
        
        for alpha in args.alpha:
            if alpha in results:
                f.write(f"Alpha = {alpha:.3f}:\n")
                f.write(f"  Parameters merged: {results[alpha].get('param_count', 'N/A')}\n")
                f.write(f"  Output path: {results[alpha].get('output_path', 'N/A')}\n")
                f.write("\n")
        
        f.write("Notes:\n")
        f.write("- Alpha = 0.0: Pure zero-shot model\n")
        f.write("- Alpha = 1.0: Pure fine-tuned model\n")
        f.write("- Alpha = 0.5: Balanced interpolation\n")
        f.write("- Compatible with original Wise-FT methodology\n")
    
    print(f"âœ… Analysis reports saved:")
    print(f"   ğŸ“„ {report_path}")
    print(f"   ğŸ“„ {summary_path}")

def main():
    parser = argparse.ArgumentParser(description='Wise-FT for LoRA Adapters')
    
    # ì›ë³¸ wise_ftì™€ ìœ ì‚¬í•œ ì¸ì êµ¬ì¡°
    parser.add_argument('--zeroshot_path', type=str, default=None,
                        help='Path to zero-shot LoRA adapter (optional, defaults to all zeros)')
    parser.add_argument('--finetuned_path', type=str, required=True,
                        help='Path to fine-tuned LoRA adapter')
    parser.add_argument('--save', type=str, required=True,
                        help='Output directory for merged models')
    parser.add_argument('--alpha', type=float, nargs='+', 
                        default=[0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0],
                        help='Alpha values for interpolation')
    
    # Fisher Information ê´€ë ¨ (ì›ë³¸ì˜ --fisher ì˜µì…˜ê³¼ ìœ ì‚¬)
    parser.add_argument('--fisher', type=str, nargs='*', default=None,
                        help='Enable Fisher Information weighting')
    parser.add_argument('--fisher_method', type=str, default='diagonal',
                        choices=['diagonal', 'empirical', 'uniform'],
                        help='Method for computing Fisher Information')
    parser.add_argument('--fisher_floor', type=float, default=1e-8,
                        help='Minimum Fisher Information value')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.save, exist_ok=True)
    
    try:
        results = wise_ft(args)
        print("âœ… Wise-FT completed successfully!")
        return results
        
    except Exception as e:
        print(f"\nâŒ Error during Wise-FT: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = main()
    if results is not None:
        print(f"ğŸ“Š Generated {len(results)} interpolated models")
    else:
        print("âŒ Wise-FT failed!")