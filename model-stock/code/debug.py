import torch
import json
import os
from safetensors.torch import load_file

# ê²½ë¡œ ì„¤ì •
base_model_path = '../stock_test_output/base/'
finetuned_path_1 = '../stock_test_output/first_model/'
finetuned_path_2 = '../stock_test_output/second_model/'

def quick_key_analysis():
    """í•µì‹¬ ì •ë³´ë§Œ ë¹ ë¥´ê²Œ ë¶„ì„"""
    print("ğŸ” Quick Key Analysis")
    print("=" * 50)
    
    # 1. Base model í‚¤ ìƒ˜í”Œ
    print("\n1ï¸âƒ£ Base Model Keys:")
    try:
        index_path = os.path.join(base_model_path, 'model.safetensors.index.json')
        if os.path.exists(index_path):
            with open(index_path, 'r') as f:
                index_data = json.load(f)
            weight_map = index_data.get('weight_map', {})
            
            print(f"   Total base keys: {len(weight_map)}")
            print("   Sample base keys (first 5):")
            for i, key in enumerate(list(weight_map.keys())[:5]):
                print(f"     {key}")
            print("   Sample base keys (last 5):")
            for i, key in enumerate(list(weight_map.keys())[-5:]):
                print(f"     {key}")
                
            # íŠ¹ì • íŒ¨í„´ í™•ì¸
            attention_keys = [k for k in weight_map.keys() if 'self_attn' in k]
            mlp_keys = [k for k in weight_map.keys() if 'mlp' in k]
            norm_keys = [k for k in weight_map.keys() if 'norm' in k]
            
            print(f"   - Attention layers: {len(attention_keys)}")
            print(f"   - MLP layers: {len(mlp_keys)}")
            print(f"   - Norm layers: {len(norm_keys)}")
            
        else:
            print("   âŒ No index file found")
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # 2. LoRA í‚¤ ìƒ˜í”Œ  
    print("\n2ï¸âƒ£ LoRA Keys:")
    try:
        lora_path = os.path.join(finetuned_path_1, 'adapter_model.safetensors')
        lora_weights = load_file(lora_path, device="cpu")
        
        print(f"   Total LoRA keys: {len(lora_weights)}")
        print("   Sample LoRA keys (first 5):")
        for i, key in enumerate(list(lora_weights.keys())[:5]):
            print(f"     {key}")
        
        # LoRA A/B ë¶„ì„
        lora_A_keys = [k for k in lora_weights.keys() if '.lora_A.' in k]
        lora_B_keys = [k for k in lora_weights.keys() if '.lora_B.' in k]
        
        print(f"   - LoRA A keys: {len(lora_A_keys)}")
        print(f"   - LoRA B keys: {len(lora_B_keys)}")
        
        if lora_A_keys:
            print(f"   Sample LoRA A: {lora_A_keys[0]}")
        if lora_B_keys:
            print(f"   Sample LoRA B: {lora_B_keys[0]}")
            
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # 3. í‚¤ ë§¤ì¹­ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Key Matching Test:")
    try:
        # ì‹¤ì œ ë§¤ì¹­ í…ŒìŠ¤íŠ¸
        if 'lora_weights' in locals() and 'weight_map' in locals():
            successful_mappings = 0
            failed_mappings = []
            
            for lora_key in list(lora_weights.keys())[:10]:  # ì²˜ìŒ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
                if '.lora_A.' in lora_key or '.lora_B.' in lora_key:
                    # ë³€í™˜ ì‹œë„
                    base_key_candidates = [
                        lora_key.replace('base_model.model.', '').replace('.lora_A.weight', '.weight').replace('.lora_B.weight', '.weight'),
                        lora_key.replace('base_model.model.', '').replace('.lora_A.default.weight', '.weight').replace('.lora_B.default.weight', '.weight'),
                        lora_key.replace('base_model.', '').replace('.lora_A.weight', '.weight').replace('.lora_B.weight', '.weight')
                    ]
                    
                    found = False
                    for candidate in base_key_candidates:
                        if candidate in weight_map:
                            successful_mappings += 1
                            found = True
                            break
                    
                    if not found:
                        failed_mappings.append(lora_key)
            
            print(f"   Successful mappings: {successful_mappings}")
            print(f"   Failed mappings: {len(failed_mappings)}")
            
            if failed_mappings:
                print(f"   Sample failed: {failed_mappings[0]}")
                
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # 4. ì œì™¸ íŒ¨í„´ í…ŒìŠ¤íŠ¸
    print("\n4ï¸âƒ£ Exclude Pattern Test:")
    if 'weight_map' in locals():
        exclude_patterns = [
            'embed_tokens.weight',
            'model.norm.weight', 
            'lm_head.weight',
            'norm.weight'
        ]
        
        total_keys = len(weight_map)
        excluded_count = 0
        included_count = 0
        
        for key in weight_map.keys():
            should_exclude = any(pattern in key or key.endswith(pattern) for pattern in exclude_patterns)
            if should_exclude:
                excluded_count += 1
            else:
                included_count += 1
        
        print(f"   Total keys: {total_keys}")
        print(f"   Would exclude: {excluded_count}")
        print(f"   Would include: {included_count}")
        
        # í¬í•¨ë  í‚¤ë“¤ ìƒ˜í”Œ
        included_keys = [k for k in list(weight_map.keys())[:20] 
                        if not any(pattern in k or k.endswith(pattern) for pattern in exclude_patterns)]
        
        print(f"   Sample included keys:")
        for key in included_keys[:5]:
            print(f"     {key}")

if __name__ == "__main__":
    quick_key_analysis()